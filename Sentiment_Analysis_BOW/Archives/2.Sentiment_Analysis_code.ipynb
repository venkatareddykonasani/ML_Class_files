{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.Sentiment_Analysis_code.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN+ePDuXeWs12L8uirEELew"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pi0tVr3HHLwh"},"source":["[statinfer.com](https://statinfer.com/)"]},{"cell_type":"markdown","metadata":{"id":"YsIVzVt1QPwv"},"source":["# Packages Installation"]},{"cell_type":"code","metadata":{"id":"--Pw52DbG7xf"},"source":["#!pip install nltk\n","#!pip install spacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vw1CX5Vo8Nkl"},"source":["import pandas as pd\n","import nltk\n","import spacy\n","nltk.download(\"all\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5s5RE-tnhuxP"},"source":["# Data Importing"]},{"cell_type":"code","metadata":{"id":"0R51forVQaG4"},"source":["twitter_data=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Twitter_Sentiment/Twitter_Sentiment_Data.csv\")\n","twitter_data.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GktjOUmPVXVT"},"source":["# A Single function for pre-processing"]},{"cell_type":"code","metadata":{"id":"VdtYy73t762-"},"source":["contra_Expan_Dict = {\"ain`t\": \"am not\",\"aren`t\": \"are not\",\"can`t\": \"cannot\",\"can`t`ve\": \"cannot have\",\"`cause\": \"because\",\n","\"could`ve\": \"could have\",\"couldn`t\": \"could not\",\"couldn`t`ve\": \"could not have\",\"didn`t\": \"did not\",\n","\"doesn`t\": \"does not\",\"don`t\": \"do not\",\"hadn`t\": \"had not\",\"hadn`t`ve\": \"had not have\",\"hasn`t\": \"has not\",\n","\"haven`t\": \"have not\",\"he`d\": \"he would\",\"he`d`ve\": \"he would have\",\"he`ll\": \"he will\",\"he`ll`ve\": \"he will have\",\n","\"he`s\": \"he is\",\"how`d\": \"how did\",\"how`d`y\": \"how do you\",\"how`ll\": \"how will\",\n","\"how`s\": \"how does\",\"i`d\": \"i would\",\"i`d`ve\": \"i would have\",\"i`ll\": \"i will\",\"i`ll`ve\": \"i will have\",\"i`m\": \"i am\",\n","\"i`ve\": \"i have\",\"isn`t\": \"is not\",\"it`d\": \"it would\",\"it`d`ve\": \"it would have\",\"it`ll\": \"it will\",\"it`ll`ve\": \"it will have\",\n","\"it`s\": \"it is\",\"let`s\": \"let us\",\"ma`am\": \"madam\",\"mayn`t\": \"may not\",\"might`ve\": \"might have\",\"mightn`t\": \"might not\",\n","\"mightn`t`ve\": \"might not have\",\"must`ve\": \"must have\",\"mustn`t\": \"must not\",\"mustn`t`ve\": \"must not have\",\"needn`t\": \"need not\",\"needn`t`ve\": \"need not have\",\n","\"o`clock\": \"of the clock\",\"oughtn`t\": \"ought not\",\"oughtn`t`ve\": \"ought not have\",\"shan`t\": \"shall not\",\n","\"sha`n`t\": \"shall not\",\"shan`t`ve\": \"shall not have\",\"she`d\": \"she would\",\n","\"she`d`ve\": \"she would have\",\"she`ll\": \"she will\",\"she`ll`ve\": \"she will have\",\n","\"she`s\": \"she is\",\"should`ve\": \"should have\",\"shouldn`t\": \"should not\",\"shouldn`t`ve\": \"should not have\",\"so`ve\": \"so have\",\"so`s\": \"so is\",\n","\"that`d\": \"that would\",\"that`d`ve\": \"that would have\",\"that`s\": \"that is\",\"there`d\": \"there would\",\"there`d`ve\": \"there would have\",\"there`s\": \"there is\",\n","\"they`d\": \"they would\",\"they`d`ve\": \"they would have\",\"they`ll\": \"they will\",\"they`ll`ve\": \"they will have\",\"they`re\": \"they are\",\"they`ve\": \"they have\",\n","\"to`ve\": \"to have\",\"wasn`t\": \"was not\",\" u \": \" you \",\" ur \": \" your \",\" n \": \" and \",\"won`t\": \"would not\",\n","\"dis\": \"this\",\"bak\": \"back\",\"brng\": \"bring\"}\n","\n","def expanded_form(x):\n","  if x in contra_Expan_Dict.keys():\n","    return(contra_Expan_Dict[x])\n","  else:\n","    return(x)\n","\n","from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n","spacy_stopwords.update({\"would\", \"rt\",\"like\", \"ha\", \"lol\", \"need\", \"do\"})\n","\n","import re\n","def clean_with_re(x):\n","  x=str(x)\n","  x=re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \", x) #Remove URLs\n","  x=re.sub(r'[^\\w ]+', \"\", x) # Remove Punctuation-1\n","  x=re.sub(r\"[,!@&\\'?\\.$%_]\",\" \", x) # Remove Punctuation-2\n","  x=re.sub(r\"\\d+\",\" \", x) #Remove digits\n","  return(x)\n","\n","spacy_model = spacy.load('en_core_web_sm')\n","\n","def pre_processing(input_data, text_col):\n","  input_data[\"text_col_clean\"]=input_data[text_col].apply(lambda x:str(x).lower())\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[expanded_form(t) for t in str(x).split()])\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:clean_with_re(x))\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:\" \".join([t.lemma_ for t in spacy_model(str(x))if t.lemma_ !=\"-PRON-\" ]))\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[t for t in str(x).split() if t not in spacy_stopwords ])\n","  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x: \" \".join(x) )\n","  print(input_data[[text_col,\"text_col_clean\"]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdADeVM0YH1G"},"source":["pre_processing(input_data=twitter_data, text_col=\"raw_tweet\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UBtGINfD82dP"},"source":["# WordClod"]},{"cell_type":"code","metadata":{"id":"-oVDQSZq8y1B"},"source":["#!pip install wordcloud\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","final_text=\"\".join(twitter_data[\"text_col_clean\"])\n","len(final_text)\n","\n","plt.figure(figsize = (15, 15), facecolor = None) \n","wc=WordCloud(colormap='Set2').generate(final_text)\n","plt.imshow(wc)\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"geG4E23U9XvP"},"source":["# Document Term Matrix"]},{"cell_type":"code","metadata":{"id":"WVo3d4cM9aXl"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","countvec1 = CountVectorizer(min_df= 5)\n","dtm_v1 = pd.DataFrame(countvec1.fit_transform(twitter_data['text_col_clean']).toarray(), columns=countvec1.get_feature_names(), index=None)\n","print(dtm_v1.shape)\n","dtm_v1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvlkZP7AH66T"},"source":["# Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"fks5SgrSJ04i"},"source":["## Features, Target, Train and Test Data"]},{"cell_type":"code","metadata":{"id":"pNpQLS4oJ9ti"},"source":["twitter_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgNmsfbR-FI7"},"source":["twitter_data['sentiment_label'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxfXC1bdKIIU"},"source":["dtm_v1['sentiment_label']=twitter_data['sentiment_label']\n","\n","#remove neutrals\n","dtm_v1=dtm_v1[dtm_v1['sentiment_label'] != \"neutral\"]\n","print(dtm_v1['sentiment_label'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlhNHgUAKk0h"},"source":["from sklearn.model_selection import train_test_split\n","\n","X=dtm_v1.drop(['sentiment_label'], axis=1)\n","y=dtm_v1['sentiment_label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state=33)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Zs-_pqvSqQT"},"source":["## Naive Bayes Model"]},{"cell_type":"code","metadata":{"id":"trwa_k5SR_eS"},"source":["from sklearn.naive_bayes import MultinomialNB\n","senti_model = MultinomialNB()\n","#Fitting model to our data\n","senti_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zytz7d4eSvVT"},"source":["print(\"Train Accuracy\", senti_model.score(X_train,y_train))\n","print(\"Test Accuracy\", senti_model.score(X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"65Esd2MoSxNA"},"source":["#Prediction\n","pred_sentiment=senti_model.predict(X_test)\n","print(pred_sentiment)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WvdP_pT0Hn7l"},"source":["## Predicting for new data points"]},{"cell_type":"code","metadata":{"id":"thwNU02zkIe0"},"source":["t1 =\"Awesome experience. Go for it. It is a great place\"\n","t2 =\"Very bad day for me today. I would like to forget it as soon as possible\"\n","tweet_list=[t1,t2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQzV_Cnsd9rk"},"source":["new_comment= pd.DataFrame({\"text\":tweet_list})\n","\n","#Spelling Correction\n","from textblob import TextBlob\n","new_comment[\"text_corrected\"]=new_comment[\"text\"].apply(lambda x:\"\".join(TextBlob(x).correct()))\n","pre_processing(input_data=new_comment, text_col=\"text_corrected\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OOOg493UxOb"},"source":["countvec = CountVectorizer()\n","dtm_newcomment = pd.DataFrame(countvec.fit_transform(new_comment['text_col_clean']).toarray(), columns=countvec.get_feature_names(), index=None)\n","#print(dtm_newcomment)\n","\n","dtm_v2=dtm_v1.drop([\"sentiment_label\"],axis=1)\n","dtm_newcomment_final=pd.DataFrame(columns=dtm_v2.columns.values)\n","dtm_newcomment_final=dtm_newcomment_final.append(dtm_newcomment)\n","dtm_newcomment_final=dtm_newcomment_final.fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J913oYzyuLay"},"source":["print(\"****Make sure that New DTM and old DTM have same number of columns***\")\n","print(\"New DTM Shape\", dtm_newcomment_final.shape)\n","print(\"Overall DTM Shape\",dtm_v2.shape)\n","\n","New_words=[t for t in dtm_newcomment_final.columns.values if t not in dtm_v2.columns.values]\n","print(\"Count of new words =>\", len(New_words))\n","print(\"New words are => \", New_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdEEJlhQgMw7"},"source":["result=pd.DataFrame()\n","result[\"text\"]=new_comment[\"text\"]\n","result[\"Sentiment\"] = senti_model.predict(dtm_newcomment_final)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZP5AbcH_bTQ"},"source":["#Dataset-2 : Amazon and Yelp Reviews"]},{"cell_type":"markdown","metadata":{"id":"nfeEGLFxRD4T"},"source":["Download Amazon and Yelp review data\n","https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Amazon_Yelp_Reviews/Review_Data.csv"]},{"cell_type":"markdown","metadata":{"id":"Cz74NUmJhxNz"},"source":["## Data Importing"]},{"cell_type":"code","metadata":{"id":"-zJQgq7vgiRN"},"source":["review_data=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Amazon_Yelp_Reviews/Review_Data.csv\")\n","review_data.sample(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lf1Czi9vRmSZ"},"source":["## Pre-processing"]},{"cell_type":"code","metadata":{"id":"uo_mkeNdRPJt"},"source":["pre_processing(input_data=review_data, text_col=\"Review\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7q5W4zgRj7a"},"source":["## Word Cloud"]},{"cell_type":"code","metadata":{"id":"6bkNirDjReV-"},"source":["#!pip install wordcloud\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","final_text=\"\".join(review_data[\"text_col_clean\"])\n","len(final_text)\n","\n","plt.figure(figsize = (15, 15), facecolor = None) \n","wc=WordCloud(colormap='rainbow', background_color=\"grey\").generate(final_text)\n","plt.imshow(wc)\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lc1Z7xxLSbIU"},"source":["## Document Term Matrix"]},{"cell_type":"code","metadata":{"id":"vHed1J9yRy--"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","countvec1 = CountVectorizer(min_df= 2)\n","dtm_v1 = pd.DataFrame(countvec1.fit_transform(review_data['text_col_clean']).toarray(), columns=countvec1.get_feature_names(), index=None)\n","print(dtm_v1.shape)\n","dtm_v1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Z-v1ztkTGWF"},"source":["# Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"QYnA1mXCTGWI"},"source":["## Features, Target, Train and Test Data"]},{"cell_type":"code","metadata":{"id":"Mi4jzkIKTGWJ"},"source":["review_data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UsNJyiPF2EcO"},"source":["review_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eFedUBUuTGWP"},"source":["review_data['Sentiment'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CIj6HLSTGWU"},"source":["dtm_v1['sentiment_label']=review_data['Sentiment']\n","\n","#remove neutrals\n","dtm_v1=dtm_v1[dtm_v1['sentiment_label'] != \"neutral\"]\n","print(dtm_v1['sentiment_label'].value_counts())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCYeaNcwTGWX"},"source":["from sklearn.model_selection import train_test_split\n","\n","X=dtm_v1.drop(['sentiment_label'], axis=1)\n","y=dtm_v1['sentiment_label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state=33)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji0AGQA7TGWa"},"source":["## Naive Bayes Model"]},{"cell_type":"code","metadata":{"id":"zIBsLvhnTGWb"},"source":["from sklearn.naive_bayes import MultinomialNB\n","senti_model_Amazon = MultinomialNB()\n","senti_model_Amazon.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"upACGhLRTGWe"},"source":["print(\"Train Accuracy\", senti_model_Amazon.score(X_train,y_train))\n","print(\"Test Accuracy\", senti_model_Amazon.score(X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEX9HcAZUlOl"},"source":["'''\n","Fine-tune alpha - Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n","P(W*|Y=1) = P(W*,Y=1)/P(Y=1)\n","What if W* is not present at training? \n","P(W*|Y=1)=Freq of W* present in class y=1 / Freq of class1\n","P(W*|Y=1)= 0/Freq of class1\n","Then we may need to add an alpha to both numerator and denominator \n","P(W*|Y=1) = (P(W*,Y=1)+alpha)/(P(Y=1)+(total classes)*alpha)\n","If alpha is too low or zero ==> Overfitting on train data\n","If alpha is too high ==> Then ignoring too much of training data - Underfitting\n","'''\n","senti_model_Amazon = MultinomialNB(alpha=100)\n","senti_model_Amazon.fit(X_train, y_train)\n","print(\"Train Accuracy\", senti_model_Amazon.score(X_train,y_train))\n","print(\"Test Accuracy\", senti_model_Amazon.score(X_test,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpL2NQ1dTGWi"},"source":["#Prediction\n","pred_sentiment=senti_model_Amazon.predict(X_test)\n","print(pred_sentiment)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2pdFhO5TGWk"},"source":["## Predicting for new data points"]},{"cell_type":"code","metadata":{"id":"LBG69Xg0TGWl"},"source":["t1 =\"Disappointed with the quality of the product. They need to add more value into it \"\n","t2 =\"Never order a product from here. You will regret it. \"\n","t3 = \"This restaurent is super good. I am a big fan of their menu \" \n","tweet_list=[t1,t2,t3]\n","\n","new_comment= pd.DataFrame({\"text\":tweet_list})\n","#Spelling Correction\n","from textblob import TextBlob\n","new_comment[\"text_corrected\"]=new_comment[\"text\"].apply(lambda x:\"\".join(TextBlob(x).correct()))\n","pre_processing(input_data=new_comment, text_col=\"text_corrected\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxQNeuNQTGWp"},"source":["countvec = CountVectorizer()\n","dtm_newcomment = pd.DataFrame(countvec.fit_transform(new_comment['text_col_clean']).toarray(), columns=countvec.get_feature_names(), index=None)\n","#print(dtm_newcomment)\n","#Now we need to fetch all the columns from the original DTM and fill them with zeros. Fill \"really\" and \"amazed\" with 1. \n","\n","dtm_v2=dtm_v1.drop([\"sentiment_label\"],axis=1)\n","dtm_newcomment_final=pd.DataFrame(columns=dtm_v2.columns.values)\n","dtm_newcomment_final=dtm_newcomment_final.append(dtm_newcomment)\n","dtm_newcomment_final=dtm_newcomment_final.fillna(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_CTrL-3FyRkz"},"source":["print(\"****Make sure that New DTM and old DTM have same number of columns***\")\n","print(\"New DTM Shape\", dtm_newcomment_final.shape)\n","print(\"Overall DTM Shape\",dtm_v2.shape)\n","\n","New_words=[t for t in dtm_newcomment_final.columns.values if t not in dtm_v2.columns.values]\n","print(\"Count of new words =>\", len(New_words))\n","print(\"New words are => \", New_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQgrUcZBTGWs"},"source":["result=pd.DataFrame()\n","result[\"text\"]=new_comment[\"text\"]\n","result[\"Sentiment\"] = senti_model_Amazon.predict(dtm_newcomment_final)\n","print(result)\n","print(\"In the training data - \\n 1: Positive \\n 0: Negative \\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lwXhamXgvjB"},"source":[""],"execution_count":null,"outputs":[]}]}